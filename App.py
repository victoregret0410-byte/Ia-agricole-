import os
import io
import requests
import streamlit as st
from groq import Groq
import pandas as pd
import pdfplumber

# =========================================================
# CONFIG GLOBALE
# =========================================================

APP_NAME = "üí¨ Conseiller IA ‚Äì agricole & g√©n√©ral"
APP_VERSION = "8.0.0"

st.set_page_config(
    page_title=APP_NAME,
    page_icon="üí¨",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Client Groq (cl√© dans les secrets Streamlit : GROQ_API_KEY)
client = Groq(api_key=os.getenv("GROQ_API_KEY", ""))


# =========================================================
# STYLE GLOBAL ‚Äì look type ChatGPT, tout blanc
# =========================================================

st.markdown(
    """
    <style>
    html, body, [class*="css"] {
        font-family: -apple-system, BlinkMacSystemFont, system-ui, sans-serif;
        background-color: #ffffff;
    }
    .main {
        background-color: #ffffff;
    }
    .block-container {
        padding-top: 1.2rem;
        padding-bottom: 3rem;
        max-width: 900px;
    }
    .stButton>button, .stDownloadButton>button {
        border-radius: 999px;
        padding: 0.35rem 1.2rem;
        font-weight: 600;
    }
    .chat-title {
        font-size: 2rem;
        font-weight: 700;
    }
    .chat-subtitle {
        color: #666;
        font-size: 0.9rem;
        margin-bottom: 0.6rem;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


# =========================================================
# MODES, LANGUES, MODELES
# =========================================================

LANG_OPTIONS = {
    "Fran√ßais": "fr",
    "English": "en",
    "Espa√±ol": "es",
    "Deutsch": "de",
}

# Mod√®les Groq (gratuits) ‚Äì tu peux en ajouter d'autres si tu veux
MODEL_OPTIONS = {
    "Groq ‚Äì rapide (LLaMA 3.2 3B)": {
        "id": "llama-3.2-3b-instruct",
        "temp": 0.3,
        "max_tokens": 500,
    },
    "Groq ‚Äì tr√®s pr√©cis (LLaMA 3.1 70B)": {
        "id": "llama-3.1-70b-versatile",
        "temp": 0.25,
        "max_tokens": 900,
    },
}

MODE_PROMPTS = {
    "G√©n√©ral": """
Tu es une IA de conversation g√©n√©rale, bienveillante, qui peut parler de n‚Äôimporte quel sujet
dans la limite des r√®gles de s√©curit√©. Tu restes respectueuse et neutre.
Quand tu ne sais pas, tu le dis clairement.
""",
    "Conseiller agricole": """
Tu es un conseiller agricole IA. Tu aides √† :
- raisonner les cultures (assolement, rotations, doses, charges, marges‚Ä¶),
- g√©rer les prairies et les stocks fourragers,
- am√©liorer l‚Äô√©levage (bovins, ovins, caprins, volailles‚Ä¶) sur la technique de base,
- r√©fl√©chir au travail, √† la s√©curit√©, au confort de vie.
Tu expliques calmement, comme un coll√®gue agriculteur exp√©riment√©.
""",
    "Gestion & compta": """
Tu aides √† lire les chiffres de l‚Äôexploitation : produits, charges, marges, EBE,
capacit√© de remboursement. Tu peux proposer des tableaux, des exemples de calcul,
mais tu ne remplaces pas un expert-comptable ou un conseiller de gestion.
Tu expliques chaque √©tape de calcul.
""",
    "Tech / documents": """
Tu aides √† √©crire et am√©liorer des documents (mails, courriers, rapports),
cr√©er des mod√®les de factures, de tableaux, de check-lists, des proc√©dures.
Tu fais attention √† l‚Äôorthographe et √† la clart√©.
""",
}

BASE_SYSTEM_PROMPT = """
Tu es une IA de conversation, toujours calme et respectueuse.
Tu ne fais jamais de propos offensants, haineux ou discriminants.
Tu ne donnes pas de conseils dangereux (sant√©, violence, ill√©gal‚Ä¶).

Tu expliques les choses avec :
- phrases courtes,
- vocabulaire simple,
- structure claire (titres, puces),
- quelques emojis pour aider √† lire (üåæüêÑüìäüí∂üí°‚ö†Ô∏è‚úÖ‚Ä¶).

Tu dois privil√©gier la pr√©cision et le raisonnement logique
plut√¥t que des r√©ponses vagues ou al√©atoires.
Quand tu donnes un conseil, tu expliques d‚Äôabord le raisonnement.
"""


# =========================================================
# OUTILS : lecture fichiers, m√©t√©o, mod√®les de tableaux, etc.
# =========================================================

def lire_csv(file) -> str:
    """R√©sum√© texte d'un CSV pour le contexte IA (10 lignes max)."""
    try:
        df = pd.read_csv(file)
    except Exception:
        file.seek(0)
        df = pd.read_csv(file, sep=";")
    apercu = df.head(10)
    return (
        f"Fichier CSV : {getattr(file, 'name', 'inconnu')}\n"
        f"Colonnes : {list(df.columns)}\n"
        f"Extrait (10 lignes) :\n{apercu.to_markdown(index=False)}"
    )


def lire_pdf(file) -> str:
    """Extrait les 2 premi√®res pages d'un PDF pour le contexte IA."""
    texte_total = []
    with pdfplumber.open(file) as pdf:
        for i, page in enumerate(pdf.pages):
            if i >= 2:
                break
            texte_page = page.extract_text() or ""
            texte_total.append(f"--- Page {i+1} ---\n{texte_page}")
    return (
        f"Fichier PDF : {getattr(file, 'name', 'inconnu')}\n"
        "Extraits des 2 premi√®res pages :\n" + "\n\n".join(texte_total)
    )


def generer_modele_facture_df():
    """Mod√®le simple de facture (agricole ou autre)."""
    return pd.DataFrame({
        "Date": [""],
        "N¬∞ facture": [""],
        "Client": [""],
        "Adresse client": [""],
        "SIRET / TVA client": [""],
        "Description": [""],
        "Quantit√©": [0],
        "Unit√©": [""],  # t, kg, h, u...
        "Prix unitaire HT": [0.0],
        "TVA (%)": [20],
        "Total HT": [0.0],
        "Total TTC": [0.0],
        "Mode de r√®glement": [""],
        "Date d‚Äô√©ch√©ance": [""],
    })


def generer_modeles_tableaux_gestion():
    """Mod√®les de tableaux utiles pour une ferme."""
    df_marges = pd.DataFrame(columns=[
        "Ann√©e", "Atelier / Culture", "Surface_ha / Nb t√™tes",
        "Produit total ‚Ç¨", "Charges op√©rationnelles ‚Ç¨",
        "Charges de structure ‚Ç¨", "Marge brute ‚Ç¨", "EBE ‚Ç¨",
        "Marge brute /ha ou /t√™te", "EBE /ha ou /t√™te"
    ])

    df_tresorerie = pd.DataFrame(columns=[
        "Date", "Type (encaissement / d√©caissement)", "Cat√©gorie",
        "Libell√©", "Montant ‚Ç¨", "Moyen de paiement", "Atelier", "Observation"
    ])

    df_elevage = pd.DataFrame(columns=[
        "Ann√©e", "Esp√®ce", "Atelier", "Nb animaux moyen",
        "GMQ (g/j) ou Prod. lait (kg/VL/an)",
        "Conso concentr√©s (kg/an)", "Taux de renouvellement (%)",
        "Taux de mortalit√© (%)", "Remarques techniques"
    ])

    return {
        "Suivi_marges": df_marges,
        "Tr√©sorerie": df_tresorerie,
        "Elevage": df_elevage,
    }


def get_meteo_precise(location: str, nb_villes: int = 5):
    """
    M√©t√©o pr√©cise via Open-Meteo :
    - cherche plusieurs villes proches (nb_villes),
    - renvoie la m√©t√©o d√©taill√©e pour la premi√®re
      + une liste de villes proches √† comparer.
    """
    if not location:
        return None, None, "Aucune localisation fournie."
    try:
        geo_url = "https://geocoding-api.open-meteo.com/v1/search"
        params_geo = {
            "name": location,
            "count": nb_villes,
            "language": "fr",
            "format": "json"
        }
        r_geo = requests.get(geo_url, params=params_geo, timeout=8)
        if r_geo.status_code != 200:
            return None, None, "Impossible de joindre le service de g√©ocodage m√©t√©o."

        data_geo = r_geo.json()
        if "results" not in data_geo or not data_geo["results"]:
            return None, None, f"Aucune localisation trouv√©e pour ¬´ {location} ¬ª."

        # Liste des villes propos√©es
        villes = pd.DataFrame([{
            "Nom": r["name"],
            "Pays": r.get("country", ""),
            "Lat": r["latitude"],
            "Lon": r["longitude"],
        } for r in data_geo["results"]])

        # On prend la premi√®re pour la m√©t√©o d√©taill√©e
        loc = data_geo["results"][0]
        lat = loc["latitude"]
        lon = loc["longitude"]
        nom = loc.get("name", location)
        pays = loc.get("country", "")

        meteo_url = "https://api.open-meteo.com/v1/forecast"
        params_met = {
            "latitude": lat,
            "longitude": lon,
            "hourly": "temperature_2m,precipitation",
            "daily": "temperature_2m_max,temperature_2m_min,precipitation_sum",
            "current_weather": "true",
            "timezone": "auto"
        }
        r_met = requests.get(meteo_url, params=params_met, timeout=8)
        if r_met.status_code != 200:
            return None, villes, "Impossible de joindre le service m√©t√©o."

        data_met = r_met.json()
        current = data_met.get("current_weather", {})
        daily = data_met.get("daily", {})

        df_daily = None
        try:
            df_daily = pd.DataFrame({
                "Date": daily["time"],
                "T max (¬∞C)": daily["temperature_2m_max"],
                "T min (¬∞C)": daily["temperature_2m_min"],
                "Pluie jour (mm)": daily["precipitation_sum"],
            })
        except Exception:
            pass

        info = {
            "nom": nom,
            "pays": pays,
            "current": current,
            "daily_df": df_daily
        }
        return info, villes, None
    except Exception as e:
        return None, None, f"Erreur m√©t√©o : {e}"


# =========================================================
# √âTAT : multi-conversations type ChatGPT
# =========================================================

if "conversations" not in st.session_state:
    st.session_state.conversations = []

if "current_index" not in st.session_state:
    st.session_state.current_index = 0


def creer_nouvelle_conversation(title: str, mode: str, lang: str, model_label: str):
    conv = {
        "title": title,
        "mode": mode,
        "lang": lang,
        "model": model_label,
        "messages": [
            {
                "role": "assistant",
                "content": "Salut üëã\n\nExplique-moi ta situation, on va regarder √ßa calmement."
            }
        ],
        "fichiers_contextes": [],
    }
    st.session_state.conversations.append(conv)
    st.session_state.current_index = len(st.session_state.conversations) - 1


# Premi√®re discussion par d√©faut
if not st.session_state.conversations:
    creer_nouvelle_conversation(
        "Discussion 1",
        "G√©n√©ral",
        "Fran√ßais",
        "Groq ‚Äì tr√®s pr√©cis (LLaMA 3.1 70B)",
    )


# =========================================================
# SIDEBAR : langues, modes, mod√®les, listes de chats
# =========================================================

with st.sidebar:
    st.markdown("### üí¨ Conseiller IA")
    st.caption(f"Version {APP_VERSION}")

    lang_choice = st.selectbox("Langue :", list(LANG_OPTIONS.keys()))
    mode_choice = st.selectbox("Mode :", list(MODE_PROMPTS.keys()))
    model_choice = st.selectbox("Version d‚ÄôIA :", list(MODEL_OPTIONS.keys()))

    st.markdown("---")
    if st.button("‚ûï Nouvelle discussion"):
        titre = f"{mode_choice} ‚Äì {lang_choice} #{len(st.session_state.conversations) + 1}"
        creer_nouvelle_conversation(titre, mode_choice, lang_choice, model_choice)

    st.markdown("##### Mes discussions")
    labels = [c["title"] for c in st.session_state.conversations]
    idx = st.session_state.current_index
    if idx >= len(labels):
        idx = len(labels) - 1
    selected = st.radio(
        "",
        options=list(range(len(labels))),
        format_func=lambda i: labels[i],
        index=idx,
    )
    st.session_state.current_index = selected

    st.markdown("---")
    st.markdown(
        "‚ÑπÔ∏è L‚ÄôIA utilise **Groq** (mod√®les LLaMA) : rapide et gratuit.\n"
        "Pour plus de pr√©cision, choisis le mod√®le 70B."
    )

# Conversation active
conv = st.session_state.conversations[st.session_state.current_index]

# On synchronise ce que l‚Äôutilisateur a choisi dans la sidebar
conv["mode"] = mode_choice
conv["lang"] = lang_choice
conv["model"] = model_choice


# =========================================================
# CONSTRUCTION DES MESSAGES POUR L‚ÄôIA
# =========================================================

def construire_messages(conv):
    lang_code = LANG_OPTIONS.get(conv["lang"], "fr")
    mode_prompt = MODE_PROMPTS.get(conv["mode"], "")

    messages = [
        {"role": "system", "content": BASE_SYSTEM_PROMPT},
        {
            "role": "system",
            "content": f"La langue de r√©ponse doit √™tre : {conv['lang']} (code {lang_code})."
        },
        {"role": "system", "content": mode_prompt},
    ]

    # Contexte fichiers (2 derniers seulement pour aller vite)
    if conv["fichiers_contextes"]:
        ctx = conv["fichiers_contextes"][-2:]
        contexte_text = (
            "Voici des extraits de documents fournis par l‚Äôutilisateur "
            "(tableaux, PDF, etc.). Utilise-les si c‚Äôest utile :\n\n"
            + "\n\n---\n\n".join(ctx)
        )
        messages.append({"role": "system", "content": contexte_text})

    # 10 derniers messages
    derniers = conv["messages"][-10:]
    for m in derniers:
        if m["role"] in ["user", "assistant"]:
            messages.append({"role": m["role"], "content": m["content"]})

    return messages


def appeler_modele(conv):
    model_conf = MODEL_OPTIONS[conv["model"]]
    messages_for_api = construire_messages(conv)

    try:
        completion = client.chat.completions.create(
            model=model_conf["id"],
            messages=messages_for_api,
            temperature=model_conf["temp"],
            max_tokens=model_conf["max_tokens"],
        )
        return completion.choices[0].message.content
    except Exception as e:
        msg = str(e)
        if "api_key" in msg.lower() or "authentication" in msg.lower():
            return (
                "‚ùå Je ne peux pas r√©pondre car la **cl√© GROQ_API_KEY** n‚Äôest pas valide.\n\n"
                "Va dans les *Secrets* Streamlit et v√©rifie que tu as bien :\n"
                '`GROQ_API_KEY = "gsk_........"`'
            )
        return (
            "‚ùå Impossible de contacter le mod√®le Groq pour l‚Äôinstant.\n\n"
            f"(D√©tail technique : {e})"
        )


# =========================================================
